{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pickle\n",
    "from scipy.io import arff\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "import threading\n",
    "import operator\n",
    "import collections\n",
    "from scipy.spatial import distance\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances\n",
    "INPUT_DIR = \"./Datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeletClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def fetchDistanceMeasure(self,dist_fn):\n",
    "        # Map in the function of the distance metric\n",
    "        if dist_fn == 'euclidean':\n",
    "            return euclidean_distances\n",
    "        elif dist_fn == 'manhattan':\n",
    "            return manhattan_distances\n",
    "        elif dist_fn == 'cosine':\n",
    "            return distance.cosine\n",
    "        else:\n",
    "            return distance.euclidean\n",
    "   \n",
    "        \n",
    "    def __init__(self, min_len=20, max_len=20, n_threads=2, shapelet_count=10, dist_fn = 'euclidean'):\n",
    "        self.min_len = min_len\n",
    "        self.max_len = max_len\n",
    "        self.n_threads = n_threads\n",
    "        self.k = 1\n",
    "        self.dist_fn = self.fetchDistanceMeasure(dist_fn)\n",
    "        self.shapelet_count = shapelet_count\n",
    "        self.shapelet_dict = {}\n",
    "        self.shapelet_dict1 = {}\n",
    "    \n",
    "    def save_model(self):\n",
    "        shapelet_pickle = open(\"shapelet.pickle\",\"wb\")\n",
    "        pickle.dump(self.shapelet_dict1, shapelet_pickle)\n",
    "        shapelet_pickle.close()\n",
    "    \n",
    "    def load_model(self,pickle_dir):\n",
    "        pickle_in = open(pickle_dir,\"rb\")\n",
    "        self.shapelet_dict1 = pickle.load(pickle_in)\n",
    "        \n",
    "#     def fit(self, X, y):\n",
    "#         self.time_series = X\n",
    "#         self.labels = y\n",
    "#         candidates = self.generate_candidates()\n",
    "#         split_candidates = np.array_split(candidates,self.n_threads)\n",
    "#         threads = [threading.Thread(target=self.find_best_shapelet_worker, args=[pd.DataFrame(split_candidate),]) for split_candidate in split_candidates]\n",
    "#         for thread in threads:\n",
    "#             thread.start()\n",
    "#         for thread in threads:\n",
    "#             thread.join()\n",
    "#         return self\n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        self.time_series = X\n",
    "        self.classes = np.unique([label for label in y])\n",
    "        for _class in self.classes:\n",
    "            labels = []\n",
    "            print(\" Fetch Shapelets for \",_class)\n",
    "            for label in y:\n",
    "                if label == _class: labels.append(1)\n",
    "                else: labels.append(0) \n",
    "            self.labels = pd.Series(labels)\n",
    "            candidates = self.generate_candidates()\n",
    "            split_candidates = np.array_split(candidates,self.n_threads)\n",
    "            threads = [threading.Thread(target=self.find_best_shapelet_worker, args=[pd.DataFrame(split_candidate),]) for split_candidate in split_candidates]\n",
    "            for thread in threads:\n",
    "                thread.start()\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "            self.shapelet_dict1[_class] = self.shapelet_dict\n",
    "            self.shapelet_dict = {}\n",
    "        return self\n",
    "    \n",
    "    def visualise_shapelet(self):\n",
    "        for _class in self.classes:\n",
    "            count = 0\n",
    "            print( \"FOR CLASS \",_class )\n",
    "            shapelet_dict = self.shapelet_dict1[_class]\n",
    "            for ig in sorted(shapelet_dict,reverse=True)[:self.shapelet_count]:\n",
    "                for shapelet in shapelet_dict[ig]:\n",
    "                    if count > self.shapelet_count:\n",
    "                        return\n",
    "                    print(\"Shapelet:\", shapelet)\n",
    "                    #print(\"Distance:\",shapelet_dict[shapelet][1])\n",
    "                    print(\"Information Gain:\",ig)\n",
    "                    plt.plot(shapelet[0])\n",
    "                    plt.show()\n",
    "                    count += 1\n",
    "\n",
    "    def predict(self, X, k=1, classifier=\"knn\"):\n",
    "        \n",
    "        self.classes = [1,2]\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self, ['shapelet_dict1'])\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        # Output\n",
    "        y = list()\n",
    "        \n",
    "        # Map in the function of the distance metric\n",
    "        if classifier == 'dtree':\n",
    "            for x in X:\n",
    "                neighbours = {}\n",
    "                i=0\n",
    "                votes = {}\n",
    "                for _class in self.classes:\n",
    "                    shapelet_dict = self.shapelet_dict1[_class]\n",
    "                    for ig in sorted(shapelet_dict,reverse=True)[:self.shapelet_count]:\n",
    "                        for shapelet in shapelet_dict[ig]:\n",
    "                            dist,idx = self.subsequence_distance(x, shapelet[0])\n",
    "                            if (dist < shapelet[1]):\n",
    "                                y.append(_class)\n",
    "                            else:\n",
    "                                y.append(2.0)\n",
    "                            break       \n",
    "                        break\n",
    "                    break\n",
    "        elif classifier == 'dtree1':\n",
    "            for x in X:\n",
    "                neighbours = {}\n",
    "                i=0\n",
    "                votes = {}\n",
    "                for _class in self.classes:\n",
    "                    shapelet_dict = self.shapelet_dict1[_class]\n",
    "                    for ig in sorted(shapelet_dict,reverse=True)[:1]:\n",
    "                        for shapelet in shapelet_dict[ig]:\n",
    "                            dist,idx = self.subsequence_distance(x, shapelet[0])\n",
    "                            if (dist < shapelet[1]):\n",
    "                                y.append(_class)\n",
    "                            break\n",
    "                    break\n",
    "        elif classifier == 'd_tree':\n",
    "            for x in X:\n",
    "                neighbours = {}\n",
    "                i=0\n",
    "                votes = {}\n",
    "                no_shapelets = len(self.classes) - 2\n",
    "                for idx,_class in enumerate(self.classes):\n",
    "                    print(idx,no_shapelets)\n",
    "                    shapelet_dict = self.shapelet_dict1[_class]\n",
    "                    found = False\n",
    "                    for ig in sorted(shapelet_dict,reverse=True)[:1]:\n",
    "                        for shapelet in shapelet_dict[ig]:\n",
    "#                             print(\"yo\",idx,no_shapelets)\n",
    "                            dist,split_idx = self.subsequence_distance(x, shapelet[0])\n",
    "                            if (dist < shapelet[1]):\n",
    "                                y.append(_class)\n",
    "                                found = True\n",
    "#                                 print(\"Found\") \n",
    "                            elif (idx == no_shapelets):\n",
    "                                y.append(self.classes[idx+1])\n",
    "#                                 print(\"Else Found\")\n",
    "                                found = True\n",
    "                            break\n",
    "                    if(found): \n",
    "#                         print(\"Breaking since found\")\n",
    "                        break        \n",
    "\n",
    "        else:\n",
    "            for x in X:\n",
    "                neighbours = {}\n",
    "                i=0\n",
    "                votes = {}\n",
    "                for _class in self.classes:\n",
    "                    shapelet_dict = self.shapelet_dict1[_class]\n",
    "                    for ig in sorted(shapelet_dict,reverse=True)[:self.shapelet_count]:\n",
    "                        for shapelet in shapelet_dict[ig]:\n",
    "                            dist,idx = self.subsequence_distance(x, shapelet[0])\n",
    "                            if dist not in neighbours:\n",
    "                                neighbours[dist] = _class\n",
    "                for neighbour in sorted(neighbours):\n",
    "                    if neighbours[neighbour] not in votes:\n",
    "                        votes[neighbours[neighbour]] = 1\n",
    "                    else:\n",
    "                        votes[neighbours[neighbour]] += 1\n",
    "                    i += 1\n",
    "                    if i > k:\n",
    "                        break\n",
    "                y.append(max(votes.items(), key=operator.itemgetter(1))[0])    \n",
    "        return y                \n",
    "    \n",
    "#     def search_label(self,time_serie, shapelet, split_point):\n",
    "#         dist,idx = self.subsequence_distance(time_serie,shapelet)\n",
    "#         if(idx < split_point):\n",
    "#             search_label(time_serie[:idx],shapelet,split_point)\n",
    "#         else:\n",
    "#             search_label(time_serie[idx:],shapelet,split_point)\n",
    "#     def predict(self, X):\n",
    "\n",
    "#         # Check is fit had been called\n",
    "#         check_is_fitted(self, ['shapelet_dict1'])\n",
    "\n",
    "#         # Input validation\n",
    "#         X = check_array(X)\n",
    "#         # Output\n",
    "#         y = list()\n",
    "#         for x in X:\n",
    "#             y_flag = False\n",
    "#             for _class in self.classes:\n",
    "#                 shapelet_dict = self.shapelet_dict1[_class]\n",
    "#                 for shapelet in sorted(shapelet_dict,reverse=True)[:self.shapelet_count]:\n",
    "#                     dist,idx = self.subsequence_distance(x, shapelet_dict[shapelet][0])\n",
    "#                       if(dist < shapelet_dict[shapelet][3]):\n",
    "# #                     if (dist <= shapelet_dict[shapelet][3]):\n",
    "# #                         y.append(_class)\n",
    "# #                         y_flag=True\n",
    "# #                         break\n",
    "# #                 if y_flag == True: \n",
    "# #                     break       \n",
    "#         return y\n",
    "    \n",
    "    def generate_candidates(self):\n",
    "        candidates, cand_len = [], self.max_len\n",
    "        while cand_len >= self.min_len:\n",
    "            for time_serie, label in zip(self.time_series,self.labels):\n",
    "                for k in range(len(time_serie)-cand_len+1):\n",
    "                    candidates.append((time_serie[k:k+cand_len], label))\n",
    "            cand_len -= 1\n",
    "        return pd.DataFrame(candidates)\n",
    "\n",
    "    def manhattan_distance(self, x, y, min_dist=float('inf')):\n",
    "        dist = np.sum(self.dist_fn([x,y]))\n",
    "        if dist >= min_dist: \n",
    "            return None\n",
    "        return dist\n",
    "\n",
    "    def subsequence_distance(self, time_serie, sub_seq):\n",
    "        if len(sub_seq) < len(time_serie):\n",
    "            min_dist, min_idx = float(\"inf\"), 0\n",
    "            for i in range(len(time_serie)-len(sub_seq)+1):\n",
    "                dist = self.manhattan_distance(sub_seq, time_serie[i:i+len(sub_seq)], min_dist)\n",
    "                if dist is not None and dist < min_dist: \n",
    "                    min_dist, min_idx = dist, i\n",
    "            return min_dist, min_idx\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    def calculate_entropy(self, probabilities):\n",
    "        return sum([-prob * np.log(prob)/np.log(2) if prob != 0 else 0 for prob in probabilities])\n",
    "\n",
    "    def calculate_dict_entropy(self, distances):\n",
    "        count = {}\n",
    "        for distance in distances:\n",
    "            if distance[1] in count:\n",
    "                count[distance[1]] += 1\n",
    "            else: \n",
    "                count[distance[1]] = 1\n",
    "        return self.calculate_entropy(np.divide(list(count.values()), float(sum(list(count.values())))))\n",
    "\n",
    "    def check_candidate(self,shapelet):\n",
    "        distances = {} \n",
    "        for time_serie,label in zip(self.time_series,self.labels):\n",
    "            dist, idx = self.subsequence_distance(time_serie, shapelet)\n",
    "            if dist is not None:\n",
    "                distances[dist] = [(time_serie, label)] if dist not in distances else distances[dist].append((time_serie, label))   \n",
    "        ordered_dict = collections.OrderedDict(sorted(distances.items()))\n",
    "        return self.find_best_split_dist(ordered_dict)\n",
    "\n",
    "    def find_best_split_dist(self, dist_dict):\n",
    "        distance_values = list(itertools.chain.from_iterable(list(dist_dict.values())))\n",
    "        prior_entropy = self.calculate_dict_entropy(distance_values)\n",
    "        best_distance, max_ig = 0, 0\n",
    "        best_split = 0\n",
    "        best_left, best_right = None, None\n",
    "        for split_distance in dist_dict:\n",
    "            split_left = []\n",
    "            split_right = []\n",
    "            for distance in dist_dict:\n",
    "                if distance <= split_distance: \n",
    "                    split_left.extend(dist_dict[distance])\n",
    "                else: \n",
    "                    split_right.extend(dist_dict[distance])\n",
    "            ig = prior_entropy - (float(len(split_left))/float(len(distance_values))*self.calculate_dict_entropy(split_left) + float(len(split_right))/float(len(distance_values)) * self.calculate_dict_entropy(split_right))\n",
    "            if ig > max_ig: \n",
    "                best_distance, max_ig, best_left, best_right = split_distance, ig, split_left, split_right\n",
    "        return max_ig, best_distance, best_left, best_right\n",
    "    \n",
    "    def find_best_shapelet_worker(self,candidates):\n",
    "        for candidate in candidates.values:\n",
    "            gain, dist, left, right = self.check_candidate(candidate[0])\n",
    "            if gain not in self.shapelet_dict:\n",
    "                self.shapelet_dict[gain]=[(candidate[0],dist)]\n",
    "            else:\n",
    "                self.shapelet_dict[gain].append((candidate[0],dist))\n",
    "#             print(\"Shapelet with gain Added: \",gain)\n",
    "#             print(\"Splited Data:\", [x[1] for x in left],\" & \",[x[1] for x in right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(INPUT_DIR+\"GunPoint/GunPoint_TRAIN.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "time_series, labels = df[df.columns[1:]].values,df[df.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sp = ShapeletClassifier(min_len=20,max_len=20,n_threads=1000)\n",
    "sp.fit(time_series,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.save_model()\n",
    "sp1 = ShapeletClassifier(min_len=20,max_len=20,n_threads=3000)\n",
    "sp1.load_model(\"shapelet.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#sp.visualise_shapelet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = sp1.predict(time_series,classifier=\"d_tree\")\n",
    "# predicted\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(predicted,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = sp.predict(time_series,classifier=\"dtree\")\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique([label for label in labels])\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted1 = sp.predict(time_series,classifier=\"d_tree\")\n",
    "print(predicted1)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(predicted1,labels))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = sp.predict(time_series)\n",
    "print(accuracy_score(predicted,labels))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"k\",\"accuracy\")\n",
    "for k in range(10):\n",
    "    predicted = sp.predict(time_series,k+1)\n",
    "    acc = accuracy_score(predicted,labels)\n",
    "    print(k,\":\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(data, max_len=3, min_len=3):\n",
    "    candidates, cand_len = [], max_len\n",
    "    while cand_len >= min_len:\n",
    "        time_series, labels = data.drop('target', axis=1).values, data['target']\n",
    "        for time_serie, label in zip(time_series,labels):\n",
    "            for k in range(len(time_serie)-cand_len+1):\n",
    "                candidates.append((time_serie[k:k+cand_len], label))\n",
    "        cand_len -= 1\n",
    "    return pd.DataFrame(candidates)\n",
    "\n",
    "def manhattan_distance(x, y, min_dist=float('inf')):\n",
    "    dist = np.sum(dist_manh([x,y],sum_over_features=False))\n",
    "    if dist >= min_dist: return None\n",
    "    return dist\n",
    "\n",
    "def subsequence_distance(time_series, sub_seq):\n",
    "    if len(sub_seq) < len(time_series):\n",
    "        min_dist, min_idx = float(\"inf\"), 0\n",
    "        for i in range(len(time_series)-len(sub_seq)+1):\n",
    "            dist = manhattan_distance(sub_seq, time_series[i:i+len(sub_seq)], min_dist)\n",
    "            if dist is not None and dist < min_dist: min_dist, min_idx = dist, i\n",
    "        return min_dist, min_idx\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def calculate_entropy(probabilities):\n",
    "    return sum([-prob * np.log(prob)/np.log(2) if prob != 0 else 0 for prob in probabilities])\n",
    "\n",
    "def calculate_dict_entropy(distances):\n",
    "    count = {}\n",
    "    for distance in distances:\n",
    "        if distance[1] in count:\n",
    "            count[distance[1]] += 1\n",
    "        else: \n",
    "            count[distance[1]] = 1\n",
    "    return calculate_entropy(np.divide(list(count.values()), float(sum(list(count.values())))))\n",
    "\n",
    "def check_candidate(time_series, shapelet):\n",
    "    distances = {} \n",
    "    data, labels = time_series.drop('target', axis=1).values, time_series['target']\n",
    "    for time_serie,label in zip(data,labels):\n",
    "        dist, idx = subsequence_distance(time_serie, shapelet)\n",
    "        if dist is not None:\n",
    "            distances[dist] = [(time_serie, label)] if dist not in distances else distances[dist].append((time_serie, label))        \n",
    "    return find_best_split_dist(distances)\n",
    "\n",
    "def find_best_split_dist(dist_dict):\n",
    "    distance_values = list(itertools.chain.from_iterable(list(dist_dict.values())))\n",
    "    prior_entropy = calculate_dict_entropy(distance_values)\n",
    "    best_distance, max_ig = 0, 0\n",
    "    best_left, best_right = None, None\n",
    "    for split_distance in dist_dict:\n",
    "        split_left = []\n",
    "        split_right = []\n",
    "        for distance in dist_dict:\n",
    "            if distance <= split_distance: \n",
    "                split_left.extend(dist_dict[distance])\n",
    "            else: \n",
    "                split_right.extend(dist_dict[distance])\n",
    "        ig = prior_entropy - (float(len(split_left))/float(len(distance_values))*calculate_dict_entropy(split_left) + float(len(split_right))/float(len(distance_values)) * calculate_dict_entropy(split_right))\n",
    "        if ig > max_ig: best_distance, max_ig, best_left, best_right = split_distance, ig, split_left, split_right\n",
    "    return max_ig, best_distance, best_left, best_right\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_shapelets_bf(data, max_len=10, min_len=10, plot=True, verbose=True):\n",
    "    candidates = generate_candidates(data, max_len, min_len)\n",
    "    bsf_gain, bsf_shapelet = 0, None\n",
    "    if verbose: candidates_length = len(candidates)\n",
    "    for idx, candidate in enumerate(candidates.values):\n",
    "        gain, dist, data_left, data_right = check_candidate(data, candidate[0])\n",
    "        if verbose: print(idx, '/', candidates_length, \":\", gain, dist)\n",
    "        if gain > bsf_gain:\n",
    "            bsf_gain, bsf_shapelet = gain, candidate[0]\n",
    "            if verbose:\n",
    "                print('Found new best shapelet with gain & dist:', bsf_gain, dist, [x[1] for x in data_left], \\\n",
    "                                                                                   [x[1] for x in data_right])\n",
    "            if plot:\n",
    "                plt.plot(bsf_shapelet)\n",
    "                plt.show()\n",
    "            plt.show()\n",
    "    return bsf_shapelet\n",
    "bsf_shapelet = find_shapelets_bf(df,verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidates = generate_candidates(df, 10, 10)\n",
    "# print(candidates.shape)\n",
    "# split_candidates = np.array_split(candidates,500)\n",
    "# split_candidates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing.pool import Pool\n",
    "# from multiprocessing import JoinableQueue as Queue\n",
    "# import os, sys\n",
    "\n",
    "# def find_best_shapelet_worker(data,candidates):\n",
    "#     bsf_gain, bsf_shapelet = 0, None\n",
    "#     for idx, candidate in enumerate(candidates.values):\n",
    "#         gain, dist, data_left, data_right = check_candidate(data, candidate[0])\n",
    "#         if gain > bsf_gain:\n",
    "#             bsf_gain, bsf_shapelet = gain, candidate[0]\n",
    "#     return bsf_shapelet,bsf_gain\n",
    "\n",
    "# def parallel_worker():\n",
    "#     while True:\n",
    "#         split_candidate = imageq.get()\n",
    "#         shapelet,gain = find_best_shapelet_worker(df, split_candidate) \n",
    "#         similarq.put( [shapelet, gain] )\n",
    "#         imageq.task_done()\n",
    "\n",
    "# similarq = Queue()\n",
    "# imageq = Queue()\n",
    "# candidates = generate_candidates(df, 10, 10)\n",
    "# print(\"done\")\n",
    "# split_candidates = np.array_split(candidates,500)\n",
    "# for split_candidate in split_candidates:\n",
    "#     imageq.put(pd.DataFrame(split_candidate))\n",
    "#     break\n",
    "\n",
    "# pool = Pool(5)\n",
    "# for i in range(5):\n",
    "#     pool.apply_async(parallel_worker)\n",
    "\n",
    "# imageq.join()\n",
    "\n",
    "# print(similarq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gain, best_shapelet = 0, None\n",
    "shapelet_dict = {}\n",
    "\n",
    "def find_best_shapelet_worker(data,candidates):\n",
    "    global best_gain, best_shapelet,shapelet_dict\n",
    "    for idx, candidate in enumerate(candidates.values):\n",
    "        gain, dist, data_left, data_right = check_candidate(data, candidate[0])\n",
    "        if gain not in shapelet_dict:\n",
    "            shapelet_dict[gain]=candidate[0]\n",
    "#         shapelet_dict = sorted(shapelet_dict)[:10]\n",
    "#         if\n",
    "#         if gain > best_gain:\n",
    "#             best_gain, best_shapelet = gain, candidate[0]\n",
    "            \n",
    "candidates = generate_candidates(df, 10, 10)\n",
    "print(\"done\")\n",
    "split_candidates = np.array_split(candidates,100)\n",
    "# print(split_candidates[0])\n",
    "threads = [threading.Thread(target=find_best_shapelet_worker, args=[df,pd.DataFrame(split_candidate),]) for split_candidate in split_candidates]\n",
    "# threads = [threading.Thread(target=find_best_shapelet_worker, args=[df,pd.DataFrame(split_candidates[0]),])]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "print(shapelet_dict)\n",
    "# print(\"Shapelet:\",best_shapelet)\n",
    "# print(\"Information Gain:\",best_gain)\n",
    "# plt.plot(best_shapelet)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shapelet_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_shapelet_dict = sorted(shapelet_dict,reverse=True)[:10]\n",
    "for shapelet in sorted_shapelet_dict:\n",
    "    print(\"Shapelet:\",shapelet_dict[shapelet])\n",
    "    print(\"Information Gain:\",shapelet)\n",
    "    plt.plot(shapelet_dict[shapelet])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wrap_cand(data,max_len=3,min_len=3):\n",
    "    cand_len =  max_len\n",
    "    while cand_len >= min_len:\n",
    "        time_series, labels = data.drop('target', axis=1).values, data['target']\n",
    "        for time_serie, label in zip(time_series,labels):\n",
    "            for k in range(len(time_serie)-cand_len+1):\n",
    "                gen_cand.append((time_serie[k:k+cand_len], label))\n",
    "        cand_len -= 1\n",
    "thread_count = 10\n",
    "split_dfs = np.array_split(df, thread_count)\n",
    "gen_cand = []\n",
    "threads = [threading.Thread(target=wrap_cand, args=[pd.DataFrame(split_df),]) for split_df in split_dfs]\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "candidates = pd.DataFrame(gen_cand, columns=[\"shapelet\",\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for shaplet in candidates.shapelet:\n",
    "    dist = check_candidate(df,shaplet)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_dict_entropy(histogram_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
